Hello everyone. Imagine this. 59 million people watching same cricket match at the same time. One small mistake and entire country starts tweeting hot star down. This is not a hypothetical condition. This is a real story of Disney Plus Hot Star, one of the largest OTT platforms in India, serving live sports to millions. Before 2023 ODI World Cup, Hostar had already hit 25 million concurrent streams by using just two Kubernetes clusters. That alone was insane. But then things changed for Asia Cup World Cup 2023 Hostar launch free on mobile. Yes, this Rockefeller kind of things go giving free on mobile content to millions of users with geo internet at their hand. There was no pay wall, no friction. Any country that's oppressed with cricket can watch the content. 50 to 59 millions of concurrent streams, a scale no one in the industry had ever handled before. So this kind of juggernaut statement can come with its own caveats. So let's understand here how will you handle a live 59 minute concurrent stream load on your app. um whether you are be fortunate enough to handle this much load that's another point of discussion like here we will understand if or and if like your app gets 59 million concurrent streams what issue you can get in your EKS clusters and how you should handle it like the hot star did so let's proceed during one of their load testar faced issue in which their net gateway was using up 50% of its capacity while the peak load was only 1 by10th of its value. So if peak load was increased few amounts above that net gateway would have choked like it would have reach its limits. How they were able to know it? They had enabled VPC flow locks on the net gateway ENI and by analyzing VPC flow locks they were able to know that their net gateway capacity is reaching 50% for each availability zone even with only one by 10th amount of peak traffic. So in real system you face these kind of issues like if you deploy a small project with few availability zone with some subnets you will never face this kind of issue that your net gateway outbound throughput is near exceeding. But in real world if you are fortunate enough to have your app to serve 50 million users concurrently you will face issues like this. So howstar were able to handle it. So as per the official statement we scaled out net gateway in subnet level instead of availability zone level. So they went by breaking the orthodox of keeping net gateway per availability zone only. Rather they moved and placed net gateway in each of their subnets so that each of their subnet will have an outpine outbound window for their own. And when they tested it with the peak traffic simulation then these each network each net gateway in one subnet for particularly cater to it. This particular setting worked just fine. After testing out net gateway during their load test they moved their attention towards their Kubernetes worker node. During load test they found some of their Kubernetes worker node throughput was going well beyond 8 to 9 GB per second which was not suitable for this service to work at the production conditions like here you can see the graphana screenshot of their actual cubernetes worker node throughput usage here receiver and transfer traffic for Ethernet interface you can See here lots of Kubernetes worker nodes throughput is going well beyond 8 to 9 GB per second. So to solve this particular issue what they have done is they have replaced their nodes having network throughput of 10 GB per second and also they have fixed single port on each node for a service for which throughut was exceeding. By fixing single pod on each node, the throughput of 2 to three GB per second was achieved even during peak traffic and that too you can see for very small interval of time.

Let's see another problem Hotar face while working through their Kubernetes clusters. Shortage of IP addresses. They had to serve 50 million users nationwide for World Cup 2023. But suddenly they saw one problem that their Kubernetes infra were not able to scale up parts above 350. Like they were not able to scale parts larger than 350 in number. Although they had 4,90 IP addresses per subnet and for three subnet 12,300 total IP addresses. So let's see why their pod numbers were not able to scale above 350. In their VPC CNI setting, they have minimum IP target set to 35, warm IP target set to 10. So what exactly is VPCI? VPC CNI is an add-on to Kubernetes cluster in case of AWS. This provides IP address to your part running in Kubernetes cluster. It has one setting called minimum IP target which keeps a particular amount of IP addresses reserved reserved in an particular node. So that in case that particular node need more pods that pod IP addresses can be taken from this minimum IP target. Apart from that it also has one settings warm IP target which keeps x amount of IPs in warm standby so that it may be needed within a node. So if we calculate here 12,300 IP addresses if divided by 35 IP addresses reserved on each nodes correspond to 350 nodes only and they had scale up uh need for above 400 ports. So how they able they were able to like solve it like one solution can be increasing the IP addresses here. Another solution can be decreasing the minimum IP addresses on each particular node because in their nodes not all the nodes actually need 35 ports anyway. So they actually solved this problem in both the dimensions. This started using /8 cider block which made total IP addresses equals to 49,000 in all three uh with all three subnet subnets IP address counted and they set minimum IP target to 20 warm IP target to five because of which 49,000 divided by 20 IP addresses per node gave them 2,000 plus IP addresses available. So by this they were able to uh scale without any issue above 400 ports for a given scaleup. Even with warm IP target of five per node these settings provides IPs for scaleups and with 40% IP utilization throughout World Cup 2023. So here we saw if we want to design our backend system to serve 50 million users we will run into problems like this like their Hotstar were not able to scale their ports beyond 350. Why? Despite having IP address spare, this minimum IP target limit was blocking them to scale their ports beyond 350 and during a real production traffic spikes. These kind of issue can make and break or break your system. So if we deploy EKS for our smaller RPS API or backend task, we may never face this kind of condition. But Hostar has done rigorous testing and realtime load test simulation which makes them understand that if their service need to scale up beyond 350 ports they it will not happen right away with / 20 cider block and 35 node minimum IP target limit per node. So by rigorous testing, load testing, they were able to deduce they actually need/8 cider block with 49,000 IPs in all three subnets and with minimum IP target of 20 and warm IP target of five. These kind of parameter are fine- tuned to your system with it particular use cases. There is no hard and fast rule for these kind of parameters. You have to do simulation. you have to do load testing only then you will be able to know what are the optimal parameter for your system to work under real production load. Now on this particular issue of hot star we will see a very good use case of vertical scaling. Like always you have studied that vertical scaling is a lame option. You always do horizontal scaling. It's more viable than the vertical one and less costly. But here in this case vertical scaling was the best option. So let's see what was happening. Kubernetes endpoint API was unable to scale beyond 1,000 endpoints which means it was unable to scale parts beyond 1,000 in number. So if your service needs beyond 1,000 ports, then how will you approach that particular problem statement like the their EKS cluster which needed more than 1,000 ports, they had to do vertical scaling for those parts and cap a limit of 1,000 ports for that particular EKS service because Kubernetes endpoint API was not behaving properly above 1,000 end points. So that was an amazing discussion of hotstar backend architecture uh without getting a chance to work there and actually handle 59 million concurrent users yourself like uh they we are fortunate enough that they have released their engineering blog. uh that link is provided in the description you can check that out and by that you are able to know the real backend problems in real time production traffic. So if you like this video please subscribe, like, comment and show some love that motivates me to create that kind of content again. So that's all for today. Thank you.
